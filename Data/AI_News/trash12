By Logan Kugler
Communications of the ACM,            Vol. 58 No. 5, Pages 12-14
Demand for information technology continues to grow, and computers have become an integral part of life. Most U.S. households now own multiple computing devices: 58% of American adults own a smartphone and 42% own a tablet computer, according to Pew Research. Ongoing technological developments and the Internet of Things mean more aspects of our lives are being computerized and connected, requiring ever more processing of data. Despite advances in reducing the power consumption of devices and in enhanced battery technology, today's computers continue to increase their energy use as the amount of computation increases, at a time when energy efficiency is being encouraged and demands on battery life increasingly scrutinized.
At the same time, as the drive to portability continues and hardware components become smaller, the amount of power they require to operate cannot be reduced to the same degree.
Increasingly tightly packed electronics cannot dissipate the proportionally larger amount of heat they generate, which can cause them to overheat and fail. This heat energy has to be dissipated to protect the electronics, which requires yet more energy.
One solution to the problems of such diminishing returns and of smaller devices overheating due to the growing proximity of individual components is to reduce the amount of energy used by a computing device by introducing less accuracy into its processing—a tactic known as approximate computing, where error in computation is acceptable. Reducing the energy gap between a 0 state and a 1 state takes less energy to switch from one to another, but it increases the probability of a spurious switch.
Historically, computer platform design has been a quest for ever-increasing accuracy, following the principle that every digital computation must be executed correctly. As Hadi Esmaeilzadeh and colleagues put it in their paper "General-purpose code acceleration with limited-precision analog computation," "[c]onventional techniques in energy-efficient computing navigate a design space defined by the two dimensions of performance and energy, and traditionally trade one for the other. General-purpose approximate computing explores a third dimension—error—and trades the accuracy of computation for gains in both energy and performance." They use machine learning-based transformations to accelerate approximation-tolerant programs.
V.K. Chippa and colleagues in Purdue's Integrated Systems Laboratory are exploring scalable effort design to achieve improved efficiency (power or performance) at the algorithm, architecture, and circuit levels while maintaining an acceptable (and frequently, nearly identical) quality of the overall result. Chippa et al. (2013) acknowledged that "applications are often intrinsically resilient to a large fraction of their computations being executed in an imprecise or approximate manner," described as approximate computing, or "good-enough" computing, with the aim of increasing efficiency/reducing energy consumption. The idea is that error-tolerant processes can be run on less-reliable hardware that operates faster, uses less energy, and/or is less likely to burn up.
Approximation is not a new idea, as it has been used in areas such as lossy compression and numeric computation; in fact, John von Neumann wrote a paper on it in 1956 (Probabilistic logic and the synthesis of reliable organisms from unreliable components, Automata Studies (C.E. Shannon and J. McCarthy, Eds.), Princeton University Press). According to a Computing Community Consortium blog post on the U.S. Defense Advanced Research Projects Agency (DARPA) 2014 Information Science and Technology (ISAT) Targeted Approximate Computing workshop, a number of researchers are working in this area.
The main stages of design for approximate computing are (1) identifying those elements of an application that can tolerate error, (2) calculating the extent of error that can be tolerated, (3) discovering performance or energy savings, and (4) executing the instruction.
1. Where can errors be tolerated?
First, the kernels where error can be tolerated need to be identified. It is hugely time-consuming to identify all the combinations and their computational accuracy, plus the potential energy savings.
The early research of Sasa Misailovic and his colleagues at the Massachusetts Institute of Technology (MIT) Computer Science and Artificial Intelligence Laboratory focused on enabling programs to perform less work and therefore trade accuracy for faster, or more energy-efficient, execution. The team delivered compiler transformations that, for example, skipped regions of code that "are time-consuming, but do not substantially affect the accuracy of the program's result," says Misailovic.
At the 2013 Object-Oriented Programming, Systems, Languages and Applications (OOPSLA) conference, the MIT team unveiled Rely ( http://mcarbin.github.io/rely/ ), a language developed to indicate which instructions can be processed by less-reliable hardware, to a specified probability.
2. What is the tolerable error?
Introducing deliberate errors goes against the grain, but a certain degree of inaccuracy can be tolerated by the user in certain aspects of programming. One example is video rendering, where the eye and brain fill in any missing pixels. Other applications where a certain percentage of error can be tolerated without affecting the quality of the result as far as the enduser is concerned include:
wearable electronics
voice recognition
scene reconstruction
Web search
fraud detection
tracking tags and GPS
audio, image, and video processing and compression (as in Xbox and PS3 videogaming).
The common factor here is that 100% accuracy is not needed, so there is no need to waste energy computing it. But how much error is too much?
At last year's OOPSLA conference, the same MIT team presented a system called Chisel ( http://groups.csail.mit.edu/pac/chisel/ ), a simulation program that identifies elements of programming that can tolerate error, extending Rely's analysis approach. Chisel can calculate how much error can be tolerated, evaluating the percentage of improperly rendered pixels at which the user will notice an error.
As their contribution to the issue, Chippa and other members of the Purdue group proposed the concept of Dynamic Effort Scaling, leveraging error resilience to increase efficiency. Recognition and mining (RM) are emerging computer processing capabilities anticipated on future multi-core and many-core computing platforms. To close the gap between the high computational needs of RM applications and the capabilities of the platforms, the Purdue group revealed at the International Symposium on Low Power Electronics and Design (ISLPED '14) its proposed energy-efficient Stochastic Recognition and Mining (StoRM) processor, which the group said will lead to energy savings with minimal quality loss.
Luis Ceze and his colleagues at the University of Washington (UW) have been working on approximate computing for more than five years, using a more coarsely grained approach to approximation than other researchers. One unique aspect of their work is hardware-software co-design for approximate computing. Control in modern processors accounts for a significant fraction of hardware resources (at least 50%), which fundamentally limits approximation savings. The UW team found that using limited-precision analog circuits for code acceleration, through a neural approach, is both feasible and beneficial for approximation-tolerant applications. The UW group's hardware model—SNNAP (systolic neural network accelerator in programmable logic)—assesses the effect of approximation output. It works with the neural network, accelerating approximate code, removing the need to fetch and decode individual instructions. Says Ceze, "Applications that we do well in the digital neural processing unit on FPGAs [field-programmable gate arrays] (that is, the SNNAP work) are financial analysis apps, robotics control systems, and computer vision. The analog version ( http://bit.ly/1zLkric ) also shows great promise in game physics engines and machine learning applications."
In January, the UW group published in Communications on the technique of using neural networks as general-purpose approximate accelerators. Such a system chooses a block of approximate code and learns how it behaves using a neural net; then it involves the neural net, as opposed to executing the original code.
Chisel computes how much energy can be saved. By the simple expediency of allowing errors in processing, a computer's power consumption may be reduced by 9%–19%, according to the MIT research simulations. The amounts given by other researchers vary, but there are significant savings to be had.
4. How can the instruction be executed?
The mechanism in Rely is the use of an operator that indicates the instruction can be operated on unreliable hardware in order to save energy. Previously, the period had to be inserted manually, but Chisel inserts Rely's operators automatically, also guaranteeing maximized energy savings.
The counterintuitiveness of tolerating error is a common concern among developers, according to Ceze and his colleagues at UW.
The MIT team is developing rigorous approaches to help developers understand and control the approximation technique, in the process changing the perspective of many who were initially reluctant. "Some of these developers are excited by the promise of potential performance improvements and energy savings," says Misailovic. "Others look to our techniques for a way to cope with future trends in the design of hardware circuits, which may require hardware to be less reliable. And still others see our techniques as providing novel ways to deal more effectively with software errors, which remain ubiquitous throughout our entire software infrastructure."
Conclusion
The Rely and Chisel systems and others (such as Accept, which applies a variety of approximation techniques, including hardware acceleration; Flikker, which uses critical data partitioning to save refresh power, and Precimonious, which assists developers in tuning the precision of floating-point programs) have created the possibility of off-the-shelf programming that can identify where errors can be tolerated, indicate the degree of inaccuracy of computation that can be tolerated, calculate the energy that can be saved, and insert the operators that control the computations. The savings in energy can be significant, with little noticeable loss in quality.
Research into approximate computing is still in its infancy. Error-tolerant applications combined with energy-efficient programming would seem to be the way forward, according to Baek and Chilimbi in an article about Green, their framework for energy-conscious programming.
The ongoing development of tools and frameworks continues to simplify the practical implementation of approximate computing in a number of ways, which means it would seem "good-enough" computing may be here for good.
Green: A framework for supporting energy-conscious programming using controlled approximation. Proceedings of the PLDI (pp. 198–209). http://bit.ly/1vG9NOB
Chippa, V.K., Venkataramani, S., Chakradhar, S.T., Roy, K., and Raghunathan, A. (2013, Nov.).
Approximate computing: An integrated hardware approach. Asilomar conference on Signals, Systems and Computers, Pacific Grove, CA (pp. 111–117). Washington, DC: IEEE Computer Society. http://bit.ly/1DFIvKA
Drobnis, A. (June 23, 2014).
ISAT/DARPA Workshop Targeted Approximate Computing. http://bit.ly/1xZct9a
Khan, A.I., Chatterjee, K., Wang, B., Drapcho, S., You, L., Serrao, C., Bakaul, S.R., Ramesh, R., and Salahuddin, S. (2014).
Negative capacitance in a ferroelectric capacitor. Nature Materials, December 15, 2014.
Moreau, T., Wyse, M., Nelson, J., Sampson, A., Esmaeilzadeh, H., Ceze, L., and Oskin, M. (2015).
SNNAP: Approximate computing on programmable SoCs via neural acceleration. 2015 International Symposium on High-Performance Computer Architecture. http://bit.ly/1DYYWz9
St. Amant, R., Yazdanbakhsh, A., Park, J., Thwaites, B., Esmaeilzadeh, H., Hassibi, A., Ceze, L., and Burger, D. (2014).
General-purpose code acceleration with limited-precision analog computation. Proceedings of the 41st International Symposium on Computer Architecture. http://bit.ly/1wccH7D
Author
Logan Kugler is a freelance technology writer based in Tampa FL. He has written for over 60 major publications.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and full citation on the first page. Copyright for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or fee. Request permission to publish from permissions@acm.org or fax (212) 869-0481.
 
Comment on this article
Signed comments submitted to this site are moderated and will appear if they are relevant to the topic and not abusive. Your comment will appear with your username if published. View our policy on comments
(Please sign in or create an ACM Web Account to access this feature.)
Signed comments submitted to this site are moderated and will appear if they are relevant to the topic and not abusive. Your comment will appear with your username if published. View our policy on comments
Username
An email verification has been sent to youremail@email.com
ACM veri�es that you are the owner of the email address you've provided by sending you a veri�cation message. The email message will contain a link that you must click to validate this account.
NEXT STEP: CHECK YOUR EMAIL
You must click the link within the message in order to complete the process of creating your account. You may click on the link embedded in the message, or copy the link and paste it into your browser.
Sign In for Full Access
User Name
More News & opinions
