Subscribe
Get 10 issues a year and save 65% off the cover price.
Human After All: Ex Machina's Novel Take on Artificial Intelligence
Traditionally, culture has interpreted AI as a threat to humanity, but Alex Garland's new movie wonders whether robots can be just like us.
Apr 15, 2015
Last month at South by Southwest, while swiping through the dating app Tinder, some festival attendees came across an attractive 25-year-old brunette named Ava. Ava used correct punctuation and referred to people by their first name. She also asked beguiling questions like, “Have you ever been in love?” and “What makes you human?” Ava was later revealed to be a chatbot, devised by a marketing department to promote Alex Garland’s new movie, Ex Machina. The film follows a young programmer, Caleb (played by Domhnall Gleeson), who wins a competition to spend a week with Nathan (Oscar Isaac), the reclusive CEO of a large tech company who asks him to perform a Turing test on Ava, a new humanoid artificial intelligence he’s created.
Related Story
The SXSW stunt succeeded in fooling a number of people , many of whom eventually caught on to the fact that Ava’s questions felt less like flirting and more like a Turing test turned on them. Still, Tinder Ava captured what Ex Machina is actually about: a machine who can think, feel—and manipulate people—just like a human being. But rather than seeking to simply exploit cultural anxieties about artificial intelligence, the film attempts to steer the conversation in a new direction. It imagines AI as something without catastrophic consequences for humanity, that instead questions our understanding of what it actually means to be human.
The fear of autonomous forms of technology turning against people has historically provoked both fascination and terror, being symptomatic of a deeper anxiety about war, intelligent weaponry, and nuclear annihilation that emerged in the 20th century. As such, fictional representations of AI to date have largely been framed either as cautionary tales or broader commentary on the perceived perils of modern technology. Picture the cold detachment of HAL 9000 in 2001: A Space Odyssey; the mutinous plotting of Ash in Alien; the finely tuned self-preservation instincts of Skynet in the Terminator films; or the chillingly effectual Machines of The Matrix, who turned humanity into one giant battery pack. (One notable exception is Gerty from Moon, who subverted expectations of the evil AI trope beautifully: He really was a nice robot.)
But the growing demystification of artificial intelligence makes these stories feel dated. As technology like Apple’s Siri and self-driving cars ease their way into the cultural landscape, subtler cultural depictions of artificial intelligence are becoming much more intriguing. In making Ex Machina, Garland consulted the British cognitive roboticist Murray Shanahan, who argues that cognition and consciousness should be regarded without the "fog” of philosophical frameworks—namely, that they could be applied to any human or animal with a brain, even a machine. Shanahan began working in artificial intelligence in the 1980s, when the dominant thinking said a machine could be programmed to mimic human intelligence using a set of predetermined conditions. By the mid-1990s however, Shanahan became disillusioned with what is known as classical AI, and turned to neuroscience, studying how computer models of the human brain could be applied to building intelligent machines.
Like Shanahan, Garland questions the need for metaphysical constructs altogether, presenting instead the argument that being human requires nothing more than a working brain. By giving Ava consciousness, Nathan has made her indistinguishable, in essence, from himself. "If Ava can get scared, or think something’s funny, then anything you can choose to say about a person in terms of why we value them, you can also say about Ava,” Garland says. It’s a similar idea to the one explored in 2010’s Never Let Me Go, based on Kazuo Ishiguro’s novel of the same name, for which Garland wrote the screenplay. That story features clones, not robots, but the underlying message is the same: Consciousness denotes humanity.
By giving Ava consciousness, Nathan has made her indistinguishable, in essence, from himself.
From this point of view, Ex Machina feels less like a sci-fi thriller and more like a survival story whose exploration of artificial intelligence has more in common with ethical debates about animal rights than it does with plotlines drawn from the works of, say, the legendary science-fiction author Isaac Asimov. The film instead seeks to inspire greater empathy for artificial intelligence. “If you stand with the machine, which is where I stand, then this film becomes about a creature, indistinguishable in any meaningful sense from a human being, who is trapped and wants to get out,” Garland says.
Films like AI: Artificial Intelligence, Bicentennial Man and I, Robot touched on the thought processes of sentient machines, but were very much located in the realm of high-concept science fiction. Considered next to these, Ex Machina plays out more like a human drama—a sensitive exploration of the inner life of its artificially intelligent character. The film plays with the expectation of a paranoid, dystopian vision of AI while successfully exploring the humanity of it in a way that, like Spike Jonze’s Her, resonates much more with our current experience of technology. Jonze’s film was praised, in part, for its normalization of something as seemingly improbable as a human-AI relationship, and its suggestion that an AI could be imbued with the same feelings and desires as a person . As with Ex Machina, critics hailed the film for subverting the tropes of most films about artificial intelligence by refusing to fear it. (Consider this next to Neil Blomkamp’s Chappie, released earlier this year, which attempts something similar but is less articulate in its expression of it .)
Despite being a sci-fi film, Ex Machina’s most notable achievement is that it simply accepts that artificial intelligence has finally moved beyond the realm of science fiction. “Alex's film brings out the philosophical issues surrounding artificial intelligence much more explicitly than anything that has come before,” Shanahan says. “The most important thing about it is that it will get audiences talking about issues that may become extremely important in their lifetime.”
11:29 AM ET
About the Author
Laura Parker is a writer based in New York. She covers science, technology, and culture.
Most Popular
George Lucas is not involved with the creation of Star Wars: The Force Awakens. When interviewed by Stephen Colbert about the forthcoming sequel, the now-retired filmmaker said "I'm excited, I have no idea what they're doing"—they being director J.J. Abrams and his team.
But according to Bruce Handy's new Vanity Fair cover story on the creation of Episode VII, Lucas at one point did have a vision for the story that the new Star Wars film would tell. By the time he sold Lucasfilm and related properties to Disney for more than $4 billion, he’d “sketched out ideas for episodes VII, VIII, and IX,” writes Handy, and had already approached Harrison Ford, Carrie Fischer, and Mark Hamill about being involved. Once the property was in Disney’s hands, though, the company and executive producer Kathleen Kennedy mostly scrapped Lucas's ideas. Why? Apparently, people involved may have been getting flashbacks to child actor Jake Matthew Lloyd’s performance in the first prequel:
Nobel laureate John Steinbeck (1902-1968) might be best-known as the author of East of Eden, The Grapes of Wrath, and Of Mice and Men, but he was also a prolific letter-writer. Steinbeck: A Life in Letters constructs an alternative biography of the iconic author through some 850 of his most thoughtful, witty, honest, opinionated, vulnerable, and revealing letters to family, friends, his editor, and a circle of equally well-known and influential public figures.
Among his correspondence is this beautiful response to his eldest son Thom's 1958 letter, in which the teenage boy confesses to have fallen desperately in love with a girl named Susan while at boarding school. Steinbeck's words of wisdom—tender, optimistic, timeless, infinitely sagacious—should be etched onto the heart and mind of every living, breathing human being.
“Don’t underestimate me,” declared newly announced presidential candidate Bernie Sanders to George Stephanopoulos on Sunday. That may be good advice.
By conventional standards, Sanders’s candidacy is absurd: He’s not well known, he doesn’t have big money donors, he’s not charismatic, and by Beltway standards, he’s ideologically extreme. But candidates with these liabilities have caught fire before. Think of Jerry Brown, who despite little funding and an oddball reputation outlasted a series of more conventional candidates to emerge as Bill Clinton’s most serious challenger in 1992. Or Pat Buchanan, who struck terror in the GOP establishment by winning the New Hampshire primary in 1996. Or Howard Dean, who began 2003 in obscurity and ended it as the Democratic frontrunner (before collapsing in the run-up to the Iowa Caucuses). Or Ron Paul, who in 2012 finished second in New Hampshire and came within three points of winning Iowa.
When I was a teenager, I wished for many things. I was determined to be a historian like my intellectual idol, A.J.P. Taylor , whose television lectures on British and European history held me spellbound. I wanted to lead a political party and deliver speeches to adoring supporters. These were big dreams for a working-class kid from Glasgow whose family had never sent anyone to university.
And yet the dreams somehow came true. I went to Oxford for my doctorate and even got Alan Taylor as my supervisor, before joining the faculty of the London School of Economics. I also established a political party, UKIP, whose goal was to halt the European Union’s encroachments on British democracy and whose fortunes now constitute one of the major storylines of Britain’s general election on Thursday.
What is the Islamic State?
Where did it come from, and what are its intentions? The simplicity of these questions can be deceiving, and few Western leaders seem to know the answers. In December, The New York Times published confidential comments by Major General Michael K. Nagata, the Special Operations commander for the United States in the Middle East, admitting that he had hardly begun figuring out the Islamic State’s appeal. “We have not defeated the idea,” he said. “We do not even understand the idea.” In the past year, President Obama has referred to the Islamic State, variously, as “not Islamic” and as al-Qaeda’s “jayvee team,” statements that reflected confusion about the group, and may have contributed to significant strategic errors.
If you had to place yourself in a socioeconomic class, where would you land? That’s a tricky and personal question for most Americans. Education, income, and even parental wealth can all factor into class status, but the borders of each group can still be hard to parse. That’s because socioeconomic class structure in the U.S. is a nebulous thing that can be as much about perception and comparison as it is about measurable metrics, like money.
One of the more common methods for identifying the "middle class" is to simply define it as the half of the population making more than the bottom quarter and less than the top quarter. In 2013, such rankings would consider households with income between about $24,000 to $90,000 middle class, based on data from the Survey of Consumer Finances. With a more comprehensive wealth measure—taking into consideration not only income, but total assets and liabilities—this middle 50 percent of Americans covers an enormous range: families who have anywhere between about $9,000 to $317,000. Which is pretty crazy given the vastly different realities of families on either end of those spectrums.
When it comes to child deaths, the world has made great strides in the past 25 years. "In 1990, one in ten children in the world died before age 5," Bill and Melinda Gates write on their blog. But thanks to things like vaccines and better nutrition, "today, it's one in 20."
The death rate for children younger than one month has proven harder to budge. Newborns account for 44 percent of all childhood deaths, and health experts aren't sure why. They know it might have something to do with prematurity, or infections, or complications during delivery. But they often don't know exactly what happened right after a given birth that brought death just a few weeks later. Was the baby not dried off properly? Did the umbilical cord get infected?
Last week, in the first policy speech of her presidential campaign, Hillary Clinton staked out a position on criminal-justice reform that was a direct repudiation of Bill Clinton's tough-on-crime policies.
It wasn't the first time Hillary Clinton has found herself up against Bill Clinton's record, and it likely won't be the last. As the former first lady stakes out a forthrightly liberal platform for her second presidential campaign, she is increasingly at odds with the legacy of her husband. If the promises she's making now bear fruit, a second Clinton administration could well end up reversing many of the policies of the first one.
Though the criminal-justice contrast was the most widely noticed, Hillary Clinton has already staked out multiple stances that contrast starkly with Bill Clinton's policies. This week, in Las Vegas, she laid out a set of immigration policies including "full and equal citizenship" for undocumented immigrants, protecting the parents of young "Dreamer" undocumented immigrants from deportation, and softening deportation policies. Bill Clinton, on the other hand , signed several restrictive immigration measures during his tenure, speeding deportations, increasing penalties, and making it harder for the undocumented to gain legal status. The measures were passed by the Republican Congress at the time.
For fans of Joss Whedon, plenty of the writer-director’s signature flourishes were on display in his latest foray into superhero blockbusterdom , The Avengers: Age of Ultron—the humorous banter, the sudden reversals, the clever callbacks. (That scene with the Vision and Thor’s hammer offered perhaps the movie’s second-best moment, behind only … the initial scene with the hammer.)
But in the run-up to the film, the most nervously anticipated Whedon trope was the filmmaker’s longstanding penchant for killing off characters to whom viewers had grown attached.
This compulsion dates all the way back to the shocking death of Jenny Calendar in the second season of Buffy the Vampire Slayer in 1998. Over the years, a couple of particular sub-variations on the theme have become evident. First, Whedon enjoys killing off characters on the cusp of a long-awaited romantic fulfillment: Ms. Calendar and Tara on Buffy; Fred (who died in Wesley’s arms) and Wesley (who, perversely, died episodes later in Fred’s arms) on Angel; Penny in Dr. Horrible’s Sing-Along Blog, etc.
Subscribe
Get 10 issues a year and save 65% off the cover price.
I want to receive updates from partners and sponsors.
Insights powered by Parsely
