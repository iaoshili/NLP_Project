{"content": "Twitter\u2019s home page features a regularly updated list of topics that are \u201ctrending,\u201d meaning that tweets about them have suddenly exploded in volume. A position on the list is highly coveted as a source of free publicity, but the selection of topics is automatic, based on a proprietary algorithm that factors in both the number of tweets and recent increases in that number.\nAt the Interdisciplinary Workshop on Information and Decision in Social Networks at MIT in November, Associate Professor Devavrat Shah and his student Stanislav Nikolov will present a new algorithm that can, with 95 percent accuracy, predict which topics will trend an average of an hour and a half before Twitter\u2019s algorithm puts them on the list \u2014 and sometimes as much as four or five hours before.\nThe algorithm could be of great interest to Twitter, which could charge a premium for ads linked to popular topics, but it also represents a new approach to statistical analysis that could, in theory, apply to any quantity that varies over time: the duration of a bus ride, ticket sales for films, maybe even stock prices.\nLike all machine-learning algorithms, Shah and Nikolov\u2019s needs to be \u201ctrained\u201d: it combs through data in a sample set \u2014 in this case, data about topics that previously did and did not trend \u2014 and tries to find meaningful patterns. What distinguishes it is that it\u2019s nonparametric, meaning that it makes no assumptions about the shape of patterns.\nLet the data decide\nIn the standard approach to machine learning, Shah explains, researchers would posit a \u201cmodel\u201d \u2014 a general hypothesis about the shape of the pattern whose specifics need to be inferred. \u201cYou\u2019d say, \u2018Series of trending things \u2026 remain small for some time and then there is a step,\u2019\u201d says Shah, the Jamieson Career Development Associate Professor in the Department of Electrical Engineering and Computer Science. \u201cThis is a very simplistic model. Now, based on the data, you try to train for when the jump happens, and how much of a jump happens.\n\u201cThe problem with this is, I don\u2019t know that things that trend have a step function,\u201d Shah explains. \u201cThere are a thousand things that could happen.\u201d So instead, he says, he and Nikolov \u201cjust let the data decide.\u201d\nIn particular, their algorithm compares changes over time in the number of tweets about each new topic to the changes over time of every sample in the training set. Samples whose statistics resemble those of the new topic are given more weight in predicting whether the new topic will trend or not. In effect, Shah explains, each sample \u201cvotes\u201d on whether the new topic will trend, but some samples\u2019 votes count more than others\u2019. The weighted votes are then combined, giving a probabilistic estimate of the likelihood that the new topic will trend.\nIn Shah and Nikolov\u2019s experiments, the training set consisted of data on 200 Twitter topics that did trend and 200 that didn\u2019t. In real time, they set their algorithm loose on live tweets, predicting trending with 95 percent accuracy and a 4 percent false-positive rate.\nShah predicts, however, that the system\u2019s accuracy will improve as the size of the training set increases. \u201cThe training sets are very small,\u201d he says, \u201cbut we still get strong results.\u201d\nKeeping pace\nOf course, the larger the training set, the greater the computational cost of executing Shah and Nikolov\u2019s algorithm. Indeed, Shah says, curbing computational complexity is the reason that machine-learning algorithms typically employ parametric models in the first place. \u201cOur computation scales proportionately with the data,\u201d Shah says.\nBut on the Web, he adds, computational resources scale with the data, too: As Facebook or Google add customers, they also add servers. So his and Nikolov\u2019s algorithm is designed so that its execution can be split up among separate machines. \u201cIt is perfectly suited to the modern computational framework,\u201d Shah says.\nIn principle, Shah says, the new algorithm could be applied to any sequence of measurements performed at regular intervals. But the correlation between historical data and future events may not always be as clear cut as in the case of Twitter posts. Filtering out all the noise in the historical data might require such enormous training sets that the problem becomes computationally intractable even for a massively distributed program. But if the right subset of training data can be identified, Shah says, \u201cIt will work.\u201d\n\u201cPeople go to social-media sites to find out what\u2019s happening now,\u201d says Ashish Goel, an associate professor of management science at Stanford University and a member of Twitter\u2019s technical advisory board. \u201cSo in that sense, speeding up the process is something that is very useful.\u201d Of the MIT researchers\u2019 nonparametric approach, Goel says, \u201cit\u2019s very creative to use the data itself to find out what trends look like. It\u2019s quite creative and quite timely and hopefully quite useful.\u201d", "tags": ["Algorithms", "Computer science and technology", "Electrical engineering and electronics", "Social networks", "Twitter", "Nonparametric models", "Statistical inference", "Trending topics", "", "", ""], "title": "Predicting what topics will trend on Twitter"}